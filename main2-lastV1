# -*- coding: utf-8 -*-
import os
import io
import json
import shutil
from multiprocessing import freeze_support
from collections import Counter
from typing import Sequence, Tuple

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, Subset
from torch.utils.tensorboard import SummaryWriter
from torchvision import transforms
from datasets import load_dataset
from tqdm import tqdm

from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix
from sklearn.model_selection import StratifiedKFold, KFold

# -----------------------------
# Env setup
# -----------------------------
os.environ.setdefault('TF_CPP_MIN_LOG_LEVEL', '2')
os.environ.setdefault('HF_HUB_DISABLE_SYMLINKS_WARNING', '1')
torch.backends.cudnn.benchmark = True

# ---- AMP compatibility (PyTorch 1.x & 2.x) ----
try:
    from torch.amp import GradScaler as GradScalerAmp, autocast as autocast_amp  # PyTorch 2.x
    AMP_IS_V2 = True
except Exception:
    from torch.cuda.amp import GradScaler as GradScalerAmp, autocast as autocast_amp  # PyTorch 1.x
    AMP_IS_V2 = False

def autocast_ctx(device: torch.device):
    if AMP_IS_V2:
        return autocast_amp(device_type=device.type, enabled=(device.type != "cpu"))
    else:
        return autocast_amp(enabled=torch.cuda.is_available())

def make_scaler(device: torch.device):
    return GradScalerAmp(enabled=(device.type == "cuda"))

# ---- TensorFlow (for TF summaries only; keep TF on CPU) ----
try:
    import tensorflow as tf
    try:
        tf.config.set_visible_devices([], 'GPU')  # donâ€™t let TF grab VRAM
    except Exception as e:
        print("TF GPU visibility config:", e)
    TF_AVAILABLE = True
except Exception as e:
    print("TensorFlow import failed; TF summaries disabled:", e)
    TF_AVAILABLE = False

# -----------------------------
# Reproducibility
# -----------------------------
def set_seed(seed=42):
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

# -----------------------------
# Device / environment
# -----------------------------
def setup_environment():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    print(f"PyTorch version: {torch.__version__}")
    if device.type == 'cuda':
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"CUDA version: {torch.version.cuda}")
        try:
            torch.cuda.empty_cache()
        except Exception as e:
            print(f"GPU cache clear error: {e}")
            device = torch.device("cpu")
    return device

# -----------------------------
# Model (OpenCLIP)
# -----------------------------
import open_clip

def load_clip_model(device, model_name="ViT-B-32", pretrained_weights="openai", verbose=False):
    model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(
        model_name=model_name, pretrained=pretrained_weights, device=device
    )
    if verbose:
        print("\nModel structure:")
        print(model)
    return model, preprocess_train, preprocess_val

def fresh_clip(device, model_name="ViT-B-32", pretrained_weights="openai"):
    """Return a fresh CLIP model instance to avoid weight leakage across runs."""
    model, _, _ = open_clip.create_model_and_transforms(
        model_name=model_name, pretrained=pretrained_weights, device=device
    )
    return model

# -----------------------------
# Freeze/unfreeze utilities
# -----------------------------
def _set_trainable(obj, flag: bool = True):
    """Make obj trainable whether it's a Module, Parameter, or Tensor."""
    if isinstance(obj, nn.Parameter):
        obj.requires_grad_(flag)
        return 1
    if isinstance(obj, torch.Tensor) and obj.requires_grad is not None:
        obj.requires_grad_(flag)
        return 1
    if isinstance(obj, nn.Module):
        cnt = 0
        for p in obj.parameters():
            p.requires_grad = flag
            cnt += 1
        return cnt
    return 0

def unfreeze_last_n_visual_blocks(visual: nn.Module, n: int = 2):
    """
    Freeze everything, then unfreeze the last n transformer blocks (if present)
    and also the post-LN and projection parameter used after the transformer.
    Works for OpenCLIP ViT backbones where `proj` may be an nn.Parameter.
    """
    # 1) freeze all
    for p in visual.parameters():
        p.requires_grad = False

    # 2) unfreeze last n resblocks if this is a ViT
    blocks = getattr(getattr(visual, "transformer", None), "resblocks", None)
    if isinstance(blocks, (nn.ModuleList, list)) and len(blocks) > 0 and n > 0:
        for blk in blocks[-n:]:
            _set_trainable(blk, True)

    # 3) also unfreeze final norm/projection if they exist
    for name in ("ln_post", "proj"):
        if hasattr(visual, name):
            _set_trainable(getattr(visual, name), True)

# -----------------------------
# Heads / wrappers
# -----------------------------
class CustomCLIPFineTuner(nn.Module):
    """
    Fine-tune last N visual blocks, plus replace head with 1/2/3-layer classifier.
    """
    def __init__(self, base_model, num_classes, num_layers=1, fine_tune_last_n=2):
        super().__init__()
        self.visual_encoder = base_model.visual
        unfreeze_last_n_visual_blocks(self.visual_encoder, n=fine_tune_last_n)

        dev = next(self.visual_encoder.parameters()).device
        dummy = torch.randn(1, 3, 224, 224, device=dev)
        with torch.no_grad():
            out_dim = self.visual_encoder(dummy).shape[1]

        layers = []
        in_dim = out_dim
        for _ in range(num_layers - 1):
            layers += [nn.Linear(in_dim, in_dim), nn.ReLU(), nn.Dropout(0.2)]
        layers += [nn.Linear(in_dim, num_classes)]
        self.classifier = nn.Sequential(*layers)

    def forward(self, images):
        feats = self.visual_encoder(images)
        return self.classifier(feats)

class NonFineTunedCLIP(nn.Module):
    """
    Frozen visual backbone; only trains the new classifier head (1/2/3 layers).
    """
    def __init__(self, base_model, num_classes, num_layers=1):
        super().__init__()
        self.visual_encoder = base_model.visual
        for p in self.visual_encoder.parameters():
            p.requires_grad = False

        dev = next(self.visual_encoder.parameters()).device
        dummy = torch.randn(1, 3, 224, 224, device=dev)
        with torch.no_grad():
            out_dim = self.visual_encoder(dummy).shape[1]

        layers = []
        in_dim = out_dim
        for _ in range(num_layers - 1):
            layers += [nn.Linear(in_dim, in_dim), nn.ReLU(), nn.Dropout(0.2)]
        layers += [nn.Linear(in_dim, num_classes)]
        self.classifier = nn.Sequential(*layers)

    def forward(self, images):
        feats = self.visual_encoder(images)
        return self.classifier(feats)

# -----------------------------
# Dataset
# -----------------------------
CLIP_MEAN = (0.48145466, 0.4578275, 0.40821073)
CLIP_STD  = (0.26862954, 0.26130258, 0.27577711)

class FashionDataset(Dataset):
    def __init__(self, data, subcategories, transform=None, augment=False):
        self.data = data
        self.subcategories = subcategories
        self.transform = transform or transforms.Compose([
            transforms.Resize(224),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=CLIP_MEAN, std=CLIP_STD),
        ])
        self.augment = augment
        self.aug = transforms.Compose([
            transforms.RandomHorizontalFlip(),
            transforms.RandomRotation(30),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
            transforms.RandomResizedCrop(224, scale=(0.8, 1.0))
        ])

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        idx = int(idx)  # HF dataset can choke on np.int64
        item = self.data[idx]
        image = item['image'].convert('RGB')
        if self.augment:
            image = self.aug(image)
        image = self.transform(image)
        label = self.subcategories.index(item['subCategory'])
        return image, label

def load_fashion_dataset():
    ds = load_dataset("ceyda/fashion-products-small")
    dataset = ds['train']
    subcategories = sorted(list(set(dataset['subCategory'])))
    return dataset, subcategories

def compute_class_weights(dataset, subcategories):
    counts = Counter(dataset['subCategory'])
    total = sum(counts.values())
    weights = [total / counts[subcat] for subcat in subcategories]
    return torch.tensor(weights, dtype=torch.float32)

# -----------------------------
# Metrics / Visualization
# -----------------------------
def compute_metrics(true_labels, predictions, average="weighted", return_cm=True):
    precision = precision_score(true_labels, predictions, average=average, zero_division=0)
    recall = recall_score(true_labels, predictions, average=average, zero_division=0)
    f1 = f1_score(true_labels, predictions, average=average, zero_division=0)
    cm = confusion_matrix(true_labels, predictions) if return_cm else None
    return precision, recall, f1, cm

def fig_from_confusion(cm, subcategories, title="Confusion Matrix"):
    fig = plt.figure(figsize=(12, 8))
    sns.heatmap(cm, annot=False, fmt="d", cmap="Blues",
                xticklabels=subcategories, yticklabels=subcategories)
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.title(title)
    fig.tight_layout()
    return fig

def tf_log_figure(tf_writer, tag, fig, step):
    if not TF_AVAILABLE or tf_writer is None:
        return
    buf = io.BytesIO()
    fig.savefig(buf, format='png', bbox_inches='tight')
    buf.seek(0)
    image = tf.image.decode_png(buf.getvalue(), channels=4)
    image = tf.expand_dims(image, 0)
    with tf_writer.as_default():
        tf.summary.image(tag, image, step=step)

@torch.no_grad()
def log_pred_vs_actual_grid(model, loader, subcategories, device, tb_writer, tf_writer,
                            tag, step, png_dir, fold_index):
    model.eval()
    images, labels = next(iter(loader))
    images = images.to(device)
    logits = model(images)
    preds = logits.argmax(dim=1).cpu().numpy()
    labels_np = labels.numpy()

    n = min(len(images), 16)
    fig, axes = plt.subplots(4, 4, figsize=(10, 10))
    for i, ax in enumerate(axes.flat[:n]):
        img = images[i].detach().cpu()
        mean = torch.tensor(CLIP_MEAN).view(3,1,1)
        std  = torch.tensor(CLIP_STD).view(3,1,1)
        img = img * std + mean
        img = img.clamp(0,1)
        ax.imshow(np.transpose(img.numpy(), (1,2,0)))
        p = subcategories[preds[i]]; t = subcategories[labels_np[i]]
        ax.set_title(f"pred: {p}\ntrue: {t}", fontsize=9,
                     color=("green" if p==t else "red"))
        ax.axis("off")
    fig.tight_layout()

    os.makedirs(png_dir, exist_ok=True)
    safe_tag = tag.replace("/", "_")
    out_path = os.path.join(png_dir, f"pred_vs_actual_{safe_tag}_fold{fold_index}_e{step}.png")
    fig.savefig(out_path, bbox_inches="tight")
    if tb_writer is not None:
        tb_writer.add_figure(f"{tag}/PredVsActual", fig, global_step=step)
    tf_log_figure(tf_writer, f"{tag}/PredVsActual", fig, step)
    plt.close(fig)
    print(f"Saved PredVsActual PNG: {out_path}")

# -----------------------------
# Cosine Similarity (class centroids)
# -----------------------------
@torch.no_grad()
def compute_and_log_similarity_matrix(
    model,
    loader: DataLoader,
    subcategories,
    device: torch.device,
    artifacts_png_dir: str,
    artifacts_csv_dir: str,
    tag_prefix: str,
    fold_index: int,
    tb_writer: SummaryWriter = None,
    tf_writer=None,
    global_step: int = None,
):
    model.eval()
    num_classes = len(subcategories)

    feat_sums = None
    counts = None
    D = None

    for images, labels in loader:
        images = images.to(device)
        labels = labels.to(device)

        feats = model.visual_encoder(images)  # (B, D)
        feats = F.normalize(feats, dim=1)

        if D is None:
            D = feats.size(1)
            feat_sums = torch.zeros(num_classes, D, device=device)
            counts = torch.zeros(num_classes, device=device)

        for c in labels.unique().tolist():
            mask = (labels == c)
            feat_sums[c] += feats[mask].sum(dim=0)
            counts[c] += mask.sum()

    counts = counts.clamp(min=1.0).unsqueeze(1)
    centroids = feat_sums / counts
    centroids = F.normalize(centroids, dim=1)

    sim = centroids @ centroids.t()
    sim_np = sim.detach().cpu().numpy()

    df = pd.DataFrame(sim_np, index=subcategories, columns=subcategories)
    safe_tag = tag_prefix.replace("/", "_")
    os.makedirs(artifacts_csv_dir, exist_ok=True)
    csv_path = os.path.join(artifacts_csv_dir, f"similarity_{safe_tag}_fold{fold_index}.csv")
    df.to_csv(csv_path, float_format="%.6f")
    print(f"Saved similarity CSV: {csv_path}")

    os.makedirs(artifacts_png_dir, exist_ok=True)
    fig = plt.figure(figsize=(12, 10))
    sns.heatmap(df, cmap="vlag", center=0.0, vmin=-1.0, vmax=1.0)
    plt.title(f"Cosine Similarity â€” {tag_prefix} â€” Fold {fold_index}")
    plt.xlabel("Class"); plt.ylabel("Class")
    fig.tight_layout()
    png_path = os.path.join(artifacts_png_dir, f"similarity_{safe_tag}_fold{fold_index}.png")
    fig.savefig(png_path, bbox_inches="tight")
    if tb_writer is not None:
        tb_writer.add_figure(f"{tag_prefix}/CosineSimilarity", fig, global_step=(0 if global_step is None else global_step))
    tf_log_figure(tf_writer, f"{tag_prefix}/CosineSimilarity", fig, (0 if global_step is None else global_step))
    plt.close(fig)
    print(f"Saved similarity PNG: {png_path}")

# -----------------------------
# Checkpointing / Zipping
# -----------------------------
def save_checkpoint(state, ckpt_path):
    os.makedirs(os.path.dirname(ckpt_path), exist_ok=True)
    torch.save(state, ckpt_path)

def load_checkpoint(ckpt_path, map_location=None):
    return torch.load(ckpt_path, map_location=map_location)

def zip_dir(src_dir, zip_stem):
    if os.path.exists(src_dir):
        os.makedirs(os.path.dirname(zip_stem), exist_ok=True)
        shutil.make_archive(zip_stem, 'zip', src_dir)
        print(f"Zipped logs to: {zip_stem}.zip")

# -----------------------------
# Train / Evaluate (with TB & TF logging + checkpointing)
# -----------------------------
class EarlyStopping:
    def __init__(self, patience=3, delta=0.0):
        self.patience = patience
        self.delta = delta
        self.best = None
        self.count = 0
        self.stop = False
    def __call__(self, score):
        if self.best is None or score > self.best + self.delta:
            self.best = score
            self.count = 0
        else:
            self.count += 1
            if self.count >= self.patience:
                self.stop = True

@torch.no_grad()
def evaluate_model_with_metrics(model, loader, criterion, subcategories, device, step_for_plots=None,
                                tb_writer: SummaryWriter=None, tf_writer=None, model_tag="Eval"):
    model.eval()
    tot_loss = 0.0
    y_true, y_pred = [], []
    for images, labels in loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        loss = criterion(outputs, labels)
        tot_loss += loss.item()
        preds = outputs.argmax(dim=1)
        y_true.extend(labels.detach().cpu().numpy())
        y_pred.extend(preds.detach().cpu().numpy())

    avg_loss = tot_loss / max(len(loader), 1)
    precision, recall, f1, cm = compute_metrics(y_true, y_pred, average="weighted", return_cm=True)

    # Confusion matrix image
    if step_for_plots is not None and cm is not None:
        fig = fig_from_confusion(cm, subcategories, f"{model_tag} Confusion Matrix")
        if tb_writer is not None:
            tb_writer.add_figure(f"{model_tag}/ConfusionMatrix", fig, global_step=step_for_plots)
        tf_log_figure(tf_writer, f"{model_tag}/ConfusionMatrix", fig, step_for_plots)
        plt.close(fig)

    return avg_loss, precision, recall, f1

def count_trainable_params(module: nn.Module) -> int:
    return sum(p.numel() for p in module.parameters() if p.requires_grad)

def train_model_with_logging(model, train_loader, val_loader, test_loader,
                             optimizer_type, model_type, head_layers,
                             subcategories, class_weights, device,
                             num_epochs=3, lr=1e-4, base_log_dir="logs/main2-lastV1/", fold_index=0,
                             resume=True, artifacts_png_dir=None, artifacts_csv_dir=None):
    # Dirs
    run_dir = os.path.join(base_log_dir, f"{model_type}_{optimizer_type}_L{head_layers}_fold_{fold_index}")
    tb_torch_dir = os.path.join(run_dir, "tb_torch")
    tb_tf_dir = os.path.join(run_dir, "tb_tf")
    os.makedirs(run_dir, exist_ok=True)
    os.makedirs(tb_torch_dir, exist_ok=True)
    os.makedirs(tb_tf_dir, exist_ok=True)

    # Writers
    tb_writer = SummaryWriter(log_dir=tb_torch_dir)
    tf_writer = tf.summary.create_file_writer(tb_tf_dir) if TF_AVAILABLE else None

    # Optimizer
    params = [p for p in model.parameters() if p.requires_grad]
    if optimizer_type.lower() == "adam":
        optimizer = optim.Adam(params, lr=lr)
    elif optimizer_type.lower() == "adamw":
        optimizer = optim.AdamW(params, lr=lr)
    else:
        raise ValueError(f"Unsupported optimizer: {optimizer_type}")

    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))
    scaler = make_scaler(device)

    # Checkpoints
    ckpt_last = os.path.join(run_dir, "checkpoint_last.pth")
    ckpt_best = os.path.join(run_dir, "checkpoint_best.pth")
    start_epoch = 0
    best_f1 = -1.0

    if resume and os.path.exists(ckpt_last):
        print(f"[Resume] Loading checkpoint: {ckpt_last}")
        data = load_checkpoint(ckpt_last, map_location=device)
        try:
            model.load_state_dict(data["model"])
            optimizer.load_state_dict(data["optimizer"])
            scaler.load_state_dict(data["scaler"])
            start_epoch = data.get("epoch", 0) + 1
            best_f1 = data.get("best_f1", -1.0)
            print(f"Resumed from epoch {start_epoch}, best_f1={best_f1:.4f}")
        except Exception as e:
            print("Resume failed (fresh start):", e)

    # Sanity: print trainable counts
    try:
        vis_trainable = count_trainable_params(getattr(model, "visual_encoder", model))
        total_trainable = count_trainable_params(model)
        print(f"[Sanity] Trainable params â€” visual: {vis_trainable:,} | total: {total_trainable:,}")
    except Exception:
        pass

    early = EarlyStopping(patience=3)
    history = []

    for epoch in range(start_epoch, num_epochs):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        pbar = tqdm(train_loader, desc=f"{model_type} {optimizer_type} L{head_layers} Fold {fold_index} | Epoch {epoch+1}/{num_epochs}")

        batches_done = 0
        for images, labels in pbar:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad(set_to_none=True)
            with autocast_ctx(device):
                outputs = model(images)
                loss = criterion(outputs, labels)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            running_loss += loss.item()
            preds = outputs.argmax(dim=1)
            total += labels.size(0)
            correct += (preds == labels).sum().item()
            batches_done += 1
            pbar.set_postfix({"loss": f"{running_loss / batches_done:.4f}",
                              "acc": f"{(100.0*correct/max(total,1)):.2f}"})

        train_loss = running_loss / max(len(train_loader), 1)
        train_acc = 100.0 * correct / max(total, 1)

        # ---- Validation ----
        step = epoch
        val_loss, vprec, vrec, vf1 = evaluate_model_with_metrics(
            model, val_loader, criterion, subcategories, device,
            step_for_plots=step, tb_writer=tb_writer, tf_writer=tf_writer,
            model_tag=f"{model_type}/{optimizer_type}/L{head_layers}/Val"
        )

        # ---- Test ----
        test_loss, tprec, trec, tf1 = evaluate_model_with_metrics(
            model, test_loader, criterion, subcategories, device,
            step_for_plots=step, tb_writer=tb_writer, tf_writer=tf_writer,
            model_tag=f"{model_type}/{optimizer_type}/L{head_layers}/Test"
        )

        # Scalars to TB
        tb_writer.add_scalar(f"{model_type}/{optimizer_type}/L{head_layers}/Train/Loss", train_loss, step)
        tb_writer.add_scalar(f"{model_type}/{optimizer_type}/L{head_layers}/Train/Accuracy", train_acc, step)
        tb_writer.add_scalar(f"{model_type}/{optimizer_type}/L{head_layers}/Val/Loss", val_loss, step)
        tb_writer.add_scalar(f"{model_type}/{optimizer_type}/L{head_layers}/Val/Precision", vprec, step)
        tb_writer.add_scalar(f"{model_type}/{optimizer_type}/L{head_layers}/Val/Recall", vrec, step)
        tb_writer.add_scalar(f"{model_type}/{optimizer_type}/L{head_layers}/Val/F1", vf1, step)
        tb_writer.add_scalar(f"{model_type}/{optimizer_type}/L{head_layers}/Test/Loss", test_loss, step)
        tb_writer.add_scalar(f"{model_type}/{optimizer_type}/L{head_layers}/Test/Precision", tprec, step)
        tb_writer.add_scalar(f"{model_type}/{optimizer_type}/L{head_layers}/Test/Recall", trec, step)
        tb_writer.add_scalar(f"{model_type}/{optimizer_type}/L{head_layers}/Test/F1", tf1, step)
        try:
            for gi, pg in enumerate(optimizer.param_groups):
                tb_writer.add_scalar(f"{model_type}/{optimizer_type}/L{head_layers}/LR/group_{gi}", pg.get("lr", 0.0), step)
        except Exception:
            pass

        if TF_AVAILABLE and tf_writer is not None:
            with tf_writer.as_default():
                tf.summary.scalar(f"{model_type}/{optimizer_type}/L{head_layers}/Train/Loss", train_loss, step=step)
                tf.summary.scalar(f"{model_type}/{optimizer_type}/L{head_layers}/Train/Accuracy", train_acc, step=step)
                tf.summary.scalar(f"{model_type}/{optimizer_type}/L{head_layers}/Val/Loss", val_loss, step=step)
                tf.summary.scalar(f"{model_type}/{optimizer_type}/L{head_layers}/Val/Precision", vprec, step=step)
                tf.summary.scalar(f"{model_type}/{optimizer_type}/L{head_layers}/Val/Recall", vrec, step=step)
                tf.summary.scalar(f"{model_type}/{optimizer_type}/L{head_layers}/Val/F1", vf1, step=step)
                tf.summary.scalar(f"{model_type}/{optimizer_type}/L{head_layers}/Test/Loss", test_loss, step=step)
                tf.summary.scalar(f"{model_type}/{optimizer_type}/L{head_layers}/Test/Precision", tprec, step=step)
                tf.summary.scalar(f"{model_type}/{optimizer_type}/L{head_layers}/Test/Recall", trec, step=step)
                tf.summary.scalar(f"{model_type}/{optimizer_type}/L{head_layers}/Test/F1", tf1, step=step)

        # Save "last" checkpoint
        save_checkpoint({
            "epoch": epoch,
            "model": model.state_dict(),
            "optimizer": optimizer.state_dict(),
            "scaler": scaler.state_dict(),
            "best_f1": best_f1
        }, ckpt_last)

        # Save "best" checkpoint (by Val F1)
        if vf1 > best_f1:
            best_f1 = vf1
            save_checkpoint({
                "epoch": epoch,
                "model": model.state_dict(),
                "optimizer": optimizer.state_dict(),
                "scaler": scaler.state_dict(),
                "best_f1": best_f1
            }, ckpt_best)

        # Record epoch
        history.append({
            "epoch": epoch+1,
            "train_loss": train_loss,
            "train_acc": train_acc,
            "val_loss": val_loss,
            "val_precision": vprec,
            "val_recall": vrec,
            "val_f1": vf1,
            "test_loss": test_loss,
            "test_precision": tprec,
            "test_recall": trec,
            "test_f1": tf1,
        })

        print(f"[{model_type}/{optimizer_type}/L{head_layers}] Epoch {epoch+1}: "
              f"TrainLoss={train_loss:.4f} Acc={train_acc:.2f} | "
              f"ValLoss={val_loss:.4f} F1={vf1:.4f} | TestLoss={test_loss:.4f} F1={tf1:.4f}")

        # Visualize predictions on the last epoch (to keep light)
        if epoch == num_epochs - 1:
            log_pred_vs_actual_grid(
                model, val_loader, subcategories, device, tb_writer, tf_writer,
                tag=f"{model_type}/{optimizer_type}/L{head_layers}/Val",
                step=step, png_dir=os.path.join(base_log_dir, "png"), fold_index=fold_index
            )

        # Early stopping on Val F1
        early(vf1)
        if early.stop:
            print("Early stopping triggered.")
            break

    # Similarity matrix on VAL split
    try:
        compute_and_log_similarity_matrix(
            model=model,
            loader=val_loader,
            subcategories=subcategories,
            device=device,
            artifacts_png_dir=artifacts_png_dir or os.path.join(base_log_dir, "png"),
            artifacts_csv_dir=artifacts_csv_dir or os.path.join(base_log_dir, "csv"),
            tag_prefix=f"{model_type}/{optimizer_type}/L{head_layers}",
            fold_index=fold_index,
            tb_writer=tb_writer,
            tf_writer=tf_writer,
            global_step=history[-1]["epoch"] if len(history) else 0,
        )
    except Exception as e:
        print("Similarity matrix computation failed:", e)

    tb_writer.close()
    if TF_AVAILABLE and tf_writer is not None:
        tf_writer.flush()

    # Zip logs
    zip_dir(run_dir, os.path.join(base_log_dir, f"{model_type}_{optimizer_type}_L{head_layers}_fold_{fold_index}_logs"))

    return history, os.path.join(run_dir, "checkpoint_best.pth")

# -----------------------------
# Plotting & CSV exports
# -----------------------------
def plot_metrics(metrics, k_folds, out_dir):
    os.makedirs(out_dir, exist_ok=True)
    folds = list(range(1, k_folds + 1))

    # Val plots
    plt.figure(figsize=(14, 10))
    plt.subplot(2, 2, 1)
    for k in metrics:
        xs = folds[:len(metrics[k]['val_loss'])]
        plt.plot(xs, metrics[k]["val_loss"], marker="o", label=f"{k} ValLoss")
    plt.xlabel("Fold"); plt.ylabel("Loss"); plt.title("Validation Loss"); plt.legend()

    plt.subplot(2, 2, 2)
    for k in metrics:
        xs = folds[:len(metrics[k]['val_precision'])]
        plt.plot(xs, metrics[k]["val_precision"], marker="o", label=f"{k} Precision")
    plt.xlabel("Fold"); plt.ylabel("Precision"); plt.title("Validation Precision"); plt.legend()

    plt.subplot(2, 2, 3)
    for k in metrics:
        xs = folds[:len(metrics[k]['val_recall'])]
        plt.plot(xs, metrics[k]["val_recall"], marker="o", label=f"{k} Recall")
    plt.xlabel("Fold"); plt.ylabel("Recall"); plt.title("Validation Recall"); plt.legend()

    plt.subplot(2, 2, 4)
    for k in metrics:
        xs = folds[:len(metrics[k]['val_f1'])]
        plt.plot(xs, metrics[k]["val_f1"], marker="o", label=f"{k} F1")
    plt.xlabel("Fold"); plt.ylabel("F1"); plt.title("Validation F1"); plt.legend()

    plt.tight_layout()
    out_path = os.path.join(out_dir, f"metrics_plot_val.png")
    plt.savefig(out_path, bbox_inches="tight"); plt.close()
    print(f"Saved plot: {out_path}")

    # Test F1 per key
    plt.figure(figsize=(8, 6))
    for k in metrics:
        xs = folds[:len(metrics[k]['test_f1'])]
        plt.plot(xs, metrics[k]["test_f1"], marker="o", label=f"{k} Test F1")
    plt.xlabel("Fold"); plt.ylabel("F1"); plt.title("Test F1"); plt.legend()
    out_path = os.path.join(out_dir, f"metrics_plot_test_f1.png")
    plt.savefig(out_path, bbox_inches="tight"); plt.close()
    print(f"Saved plot: {out_path}")

def save_metrics_nested_to_csv(metrics_nested, out_dir, filename="metrics_summary.csv"):
    os.makedirs(out_dir, exist_ok=True)
    rows = []
    num_folds = max(len(v.get("val_loss", [])) for v in metrics_nested.values()) if metrics_nested else 0
    for i in range(num_folds):
        row = {"fold": i+1}
        for key, d in metrics_nested.items():
            for met in ["val_loss", "val_precision", "val_recall", "val_f1", "test_loss", "test_precision", "test_recall", "test_f1"]:
                vals = d.get(met, [])
                row[f"{key} {met}"] = vals[i] if i < len(vals) else None
        rows.append(row)
    df = pd.DataFrame(rows)
    out_path = os.path.join(out_dir, filename)
    df.to_csv(out_path, index=False)
    print(f"Saved CSV: {out_path}")

# -----------------------------
# Rare-classâ€“aware splitting utils
# -----------------------------
def rare_aware_train_test_split(
    labels: Sequence[int],
    indices: Sequence[int],
    test_size: float = 0.10,
    seed: int = 42
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Keeps singleton classes entirely in train; stratifies only classes with >=2 samples.
    Ensures at least 1 sample remains in train for any class sent to test.
    """
    rng = np.random.RandomState(seed)
    labels = np.asarray(labels)
    indices = np.asarray(indices)

    by_class = {}
    for idx, y in zip(indices, labels[indices]):
        by_class.setdefault(y, []).append(idx)

    test_idx = []
    for y, idxs in by_class.items():
        n = len(idxs)
        if n < 2:
            continue  # keep all in train
        n_test = int(round(test_size * n))
        n_test = max(1, min(n - 1, n_test))  # at least 1 in train
        picked = rng.choice(idxs, size=n_test, replace=False)
        test_idx.extend(picked.tolist())

    test_idx = np.array(sorted(set(test_idx)))
    trainval_mask = np.ones(len(indices), dtype=bool)
    test_pos = np.isin(indices, test_idx)
    trainval_mask[test_pos] = False

    trainval_idx = indices[trainval_mask]
    return trainval_idx, test_idx

def iter_folds_trainval(
    trainval_idx: np.ndarray,
    trainval_labels: Sequence[int],
    n_splits: int = 5,
    seed: int = 42
):
    """
    Yields (train_idx_full, val_idx_full) mapped to the original dataset indices.
    Uses StratifiedKFold if feasible; else falls back to KFold.
    """
    trainval_idx = np.asarray(trainval_idx)
    y = np.asarray([trainval_labels[i] for i in range(len(trainval_idx))])

    cnts = Counter(y)
    min_count = min(cnts.values()) if len(cnts) else 0
    if min_count >= n_splits:
        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)
        splitter = skf.split(np.zeros(len(trainval_idx)), y)
        mode = "StratifiedKFold"
    else:
        n_eff = min(n_splits, len(trainval_idx))
        if n_eff < 2:
            raise ValueError(f"Not enough samples ({len(trainval_idx)}) to make even 2 folds.")
        kf = KFold(n_splits=n_eff, shuffle=True, random_state=seed)
        splitter = kf.split(np.arange(len(trainval_idx)))
        mode = f"KFold (fallback to {n_eff} folds)"

    print(f"[CV] Using {mode}. Min class count in train/val pool = {min_count}.")
    for tr_rel, va_rel in splitter:
        tr_full = trainval_idx[tr_rel]
        va_full = trainval_idx[va_rel]
        yield tr_full, va_full

# -----------------------------
# Main
# -----------------------------
if __name__ == "__main__":
    freeze_support()
    set_seed(42)
    device = setup_environment()

    # One-time preprocessors (use these across runs)
    base_clip, preprocess_train, preprocess_val = load_clip_model(
        device, model_name="ViT-B-32", pretrained_weights="openai", verbose=False
    )

    # Data
    dataset, subcategories = load_fashion_dataset()
    class_weights = compute_class_weights(dataset, subcategories)
    labels_all = [subcategories.index(s) for s in dataset['subCategory']]

    # Optional: inspect smallest classes
    cnt = Counter(labels_all)
    smallest = sorted([(k, v) for k, v in cnt.items()], key=lambda x: x[1])[:5]
    print("Smallest class counts (label_id, count):", smallest)

    # DataLoader workers / pin_memory
    num_workers = 0 if os.name == "nt" else min(8, os.cpu_count() or 0)
    pin_mem = (device.type == "cuda")

    # Hold-out test set (rare-aware), then K-Fold/Stratified K-Fold on remainder
    all_idx = np.arange(len(labels_all))
    trainval_idx, test_idx = rare_aware_train_test_split(labels_all, all_idx, test_size=0.10, seed=42)

    test_subset = Subset(dataset, test_idx.tolist())
    test_ds  = FashionDataset(test_subset, subcategories, transform=preprocess_val, augment=False)
    test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, num_workers=num_workers, pin_memory=pin_mem)

    trainval_labels = [labels_all[i] for i in trainval_idx]
    k_folds = 5

    base_log = "logs/main2-lastV1/"
    os.makedirs(base_log, exist_ok=True)
    png_dir = os.path.join(base_log, "png")
    csv_dir = os.path.join(base_log, "csv")
    os.makedirs(png_dir, exist_ok=True)
    os.makedirs(csv_dir, exist_ok=True)

    # Metrics accumulator
    metrics = {}

    # Config sweeps
    head_layer_options = [1, 2, 3]
    optimizers_list = ["adam", "adamw"]

    for head_layers in head_layer_options:
        # -------- WITH FINE-TUNING (last two visual blocks) --------
        for optimizer_type in optimizers_list:
            key = f"{optimizer_type}_fine_tuned_L{head_layers}"
            metrics[key] = {m: [] for m in ["val_loss", "val_precision", "val_recall", "val_f1",
                                            "test_loss", "test_precision", "test_recall", "test_f1"]}

            for fold_index, (train_idx, val_idx) in enumerate(
                iter_folds_trainval(trainval_idx, trainval_labels, n_splits=k_folds, seed=42), start=1):

                # Fresh CLIP backbone per run
                clip_model = fresh_clip(device, model_name="ViT-B-32", pretrained_weights="openai")

                train_subset = Subset(dataset, train_idx.tolist())
                val_subset   = Subset(dataset, val_idx.tolist())

                train_ds = FashionDataset(train_subset, subcategories, transform=preprocess_train, augment=True)
                val_ds   = FashionDataset(val_subset,   subcategories, transform=preprocess_val,   augment=False)

                train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,
                                          num_workers=num_workers, pin_memory=pin_mem)
                val_loader   = DataLoader(val_ds, batch_size=32, shuffle=False,
                                          num_workers=num_workers, pin_memory=pin_mem)

                print(f"\n=== FT({optimizer_type}) | L{head_layers} | Fold {fold_index} ===")
                fine_model = CustomCLIPFineTuner(clip_model, len(subcategories), num_layers=head_layers, fine_tune_last_n=2).to(device)
                hist_ft, _ = train_model_with_logging(
                    fine_model, train_loader, val_loader, test_loader,
                    optimizer_type=optimizer_type, model_type="fine_tuned", head_layers=head_layers,
                    subcategories=subcategories, class_weights=class_weights, device=device,
                    num_epochs=3, lr=1e-4, base_log_dir=base_log, fold_index=fold_index, resume=False,
                    artifacts_png_dir=png_dir, artifacts_csv_dir=csv_dir
                )
                last = hist_ft[-1] if hist_ft else {"val_loss": np.nan, "val_precision": np.nan, "val_recall": np.nan, "val_f1": np.nan,
                                                    "test_loss": np.nan, "test_precision": np.nan, "test_recall": np.nan, "test_f1": np.nan}
                for m in metrics[key].keys():
                    metrics[key][m].append(last[m])

        # -------- WITHOUT FINE-TUNING (frozen visual backbone) --------
        key = f"adam_non_fine_tuned_L{head_layers}"
        metrics[key] = {m: [] for m in ["val_loss", "val_precision", "val_recall", "val_f1",
                                        "test_loss", "test_precision", "test_recall", "test_f1"]}

        for fold_index, (train_idx, val_idx) in enumerate(
            iter_folds_trainval(trainval_idx, trainval_labels, n_splits=k_folds, seed=42), start=1):

            # Fresh CLIP backbone per run
            clip_model = fresh_clip(device, model_name="ViT-B-32", pretrained_weights="openai")

            train_subset = Subset(dataset, train_idx.tolist())
            val_subset   = Subset(dataset, val_idx.tolist())

            train_ds = FashionDataset(train_subset, subcategories, transform=preprocess_train, augment=True)
            val_ds   = FashionDataset(val_subset,   subcategories, transform=preprocess_val,   augment=False)

            train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,
                                      num_workers=num_workers, pin_memory=pin_mem)
            val_loader   = DataLoader(val_ds, batch_size=32, shuffle=False,
                                      num_workers=num_workers, pin_memory=pin_mem)

            print(f"\n=== Frozen (adam) | L{head_layers} | Fold {fold_index} ===")
            non_model = NonFineTunedCLIP(clip_model, len(subcategories), num_layers=head_layers).to(device)
            hist_nft, _ = train_model_with_logging(
                non_model, train_loader, val_loader, test_loader,
                optimizer_type="adam", model_type="non_fine_tuned", head_layers=head_layers,
                subcategories=subcategories, class_weights=class_weights, device=device,
                num_epochs=3, lr=1e-4, base_log_dir=base_log, fold_index=fold_index, resume=False,
                artifacts_png_dir=png_dir, artifacts_csv_dir=csv_dir
            )
            last = hist_nft[-1] if hist_nft else {"val_loss": np.nan, "val_precision": np.nan, "val_recall": np.nan, "val_f1": np.nan,
                                                  "test_loss": np.nan, "test_precision": np.nan, "test_recall": np.nan, "test_f1": np.nan}
            for m in metrics[key].keys():
                metrics[key][m].append(last[m])

    # Plots & CSV
    plot_metrics(metrics, k_folds, out_dir=png_dir)
    save_metrics_nested_to_csv(metrics, out_dir=csv_dir, filename="metrics_summary.csv")

    print("\nDone. TensorBoard logs are under each run dir:")
    print(" - PyTorch:   .../tb_torch/")
    print(" - TensorFlow .../tb_tf/")
    print("Launch: tensorboard --logdir logs/main2-lastV1/ ")
