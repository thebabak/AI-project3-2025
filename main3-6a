# -*- coding: utf-8 -*-
import os
import copy
import time
import warnings
from contextlib import nullcontext
import os
os.environ["TRANSFORMERS_NO_TF"] = "1"   # prevents transformers from importing TensorFlow
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2" # hide TF INFO+WARNING logs; use "3" to hide ERROR too
# os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"  # optional: disable oneDNN if you ever use TF

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
from torch.utils.tensorboard import SummaryWriter
from torch.nn.utils import clip_grad_value_
from sklearn.model_selection import KFold
from sklearn.metrics import (
    confusion_matrix,
    precision_recall_fscore_support,
    accuracy_score,
)
from datasets import load_dataset
import open_clip
from torch.multiprocessing import freeze_support
from transformers import CLIPModel, CLIPProcessor
from peft import LoraConfig, get_peft_model, TaskType

# -----------------------------------------------------------------------------
# Warnings
# -----------------------------------------------------------------------------
warnings.filterwarnings("ignore", message="These pretrained weights were trained with QuickGELU activation")
warnings.filterwarnings("ignore", message="Repo card metadata block was not found")

# -----------------------------------------------------------------------------
# Config
# -----------------------------------------------------------------------------
class Config:
    BASE_DIR = os.path.join("logs", "fashion_clip_experiments")
    PNG_DIR = os.path.join(BASE_DIR, "png")
    CSV_DIR = os.path.join(BASE_DIR, "csv")
    BEST_DIR = os.path.join(BASE_DIR, "best")
    FINAL_EPOCH_TAG = 999
    K_FOLDS = 5
    BATCH_SIZE = 32
    NUM_EPOCHS = 10
    VALIDATE_EVERY = 2
    ACCUMULATION_STEPS = 4
    CLIP_VALUE = 0.1  # Gradient clipping value

    @classmethod
    def setup_dirs(cls):
        for d in [cls.PNG_DIR, cls.CSV_DIR, cls.BEST_DIR]:
            os.makedirs(d, exist_ok=True)

Config.setup_dirs()

# -----------------------------------------------------------------------------
# Helpers
# -----------------------------------------------------------------------------
def to_pyints(idx_array):
    """Convert KFold (np.int64) indices to Python ints for HF Datasets compatibility."""
    if hasattr(idx_array, "tolist"):
        idx_array = idx_array.tolist()
    return [int(i) for i in idx_array]

# -----------------------------------------------------------------------------
# Dataset
# -----------------------------------------------------------------------------
class FashionDataset(torch.utils.data.Dataset):
    def __init__(self, dataset, subcategories, augment=False):
        self.dataset = dataset
        self.subcategories = subcategories
        self.augment = augment
        self.transform = self._get_transforms()

    def _get_transforms(self):
        from torchvision import transforms
        if self.augment:
            return transforms.Compose([
                transforms.RandomHorizontalFlip(),
                transforms.RandomRotation(10),
                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
                transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225])
            ])
        return transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])
        ])

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        item = self.dataset[idx]
        image = item['image'].convert('RGB')
        label = self.subcategories.index(item['subCategory'])
        return self.transform(image), label

class FashionRawPILDataset(torch.utils.data.Dataset):
    def __init__(self, dataset, subcategories):
        self.dataset = dataset
        self.subcategories = subcategories

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        item = self.dataset[idx]
        return item["image"].convert("RGB"), self.subcategories.index(item["subCategory"])

class CollateCLIP:
    def __init__(self, processor):
        self.processor = processor

    def __call__(self, batch):
        images, labels = zip(*batch)
        enc = self.processor(images=list(images), return_tensors="pt")
        return enc["pixel_values"], torch.tensor(labels, dtype=torch.long)

# -----------------------------------------------------------------------------
# Utility: determine visual output dim safely (no dummy forward in __init__)
# -----------------------------------------------------------------------------
def get_visual_out_dim(visual: nn.Module) -> int:
    out_dim = getattr(visual, "output_dim", None)
    if out_dim is not None:
        return int(out_dim)
    # Fallback single forward AFTER the module is on a device
    dev = next(visual.parameters()).device
    with torch.no_grad():
        z = torch.zeros(1, 3, 224, 224, device=dev)
        return int(visual(z).shape[1])

# -----------------------------------------------------------------------------
# Models
# -----------------------------------------------------------------------------
class FullFineTunedCLIP(nn.Module):
    def __init__(self, base_model, num_classes):
        super().__init__()
        self.visual_encoder = base_model.visual
        out_dim = get_visual_out_dim(self.visual_encoder)
        self.classifier = nn.Sequential(
            nn.Linear(out_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes),
        )

    def forward(self, images, text_inputs=None):
        return self.classifier(self.visual_encoder(images))

class TextEncoderFineTunedCLIP(nn.Module):
    def __init__(self, base_model, subcategories, device):
        super().__init__()
        self.visual_encoder = base_model.visual
        self.text_encoder = base_model.transformer
        self.tokenizer = open_clip.get_tokenizer("ViT-B-32")
        self.device = device
        self.subcategories = subcategories
        self.text_projection = base_model.text_projection
        self.positional_embedding = base_model.positional_embedding
        self.ln_final = base_model.ln_final
        self.token_embedding = base_model.token_embedding

        # Freeze visual encoder
        for p in self.visual_encoder.parameters():
            p.requires_grad = False

    def encode_text(self, token_ids):
        x = self.token_embedding(token_ids)
        x = x + self.positional_embedding
        x = x.permute(1, 0, 2)
        x = self.text_encoder(x)
        x = x.permute(1, 0, 2)
        x = self.ln_final(x)
        return x[torch.arange(x.shape[0]), token_ids.argmax(dim=-1)] @ self.text_projection

    def forward(self, images, text_inputs=None):
        image_features = self.visual_encoder(images)
        if text_inputs is None:
            text_inputs = self.tokenizer([f"a photo of {c}" for c in self.subcategories]).to(self.device)
        text_features = self.encode_text(text_inputs)
        image_features = image_features / image_features.norm(dim=-1, keepdim=True)
        text_features = text_features / text_features.norm(dim=-1, keepdim=True)
        return (image_features @ text_features.T) * 100.0

class VisionOnlyFineTunedCLIP(nn.Module):
    def __init__(self, base_model, num_classes):
        super().__init__()
        self.visual_encoder = base_model.visual
        out_dim = get_visual_out_dim(self.visual_encoder)
        self.classifier = nn.Sequential(
            nn.Linear(out_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes),
        )

    def forward(self, images, text_inputs=None):
        return self.classifier(self.visual_encoder(images))

class PartialFineTunedCLIP(nn.Module):
    def __init__(self, base_model, num_classes, freeze_percentage=0.7):
        super().__init__()
        self.visual_encoder = base_model.visual
        self.text_encoder = base_model.transformer
        self.tokenizer = open_clip.get_tokenizer("ViT-B-32")
        self.device = next(base_model.parameters()).device
        self._freeze_layers(self.visual_encoder, freeze_percentage, "visual")
        self._freeze_layers(self.text_encoder, freeze_percentage, "text")

        out_dim = get_visual_out_dim(self.visual_encoder)
        self.classifier = nn.Linear(out_dim, num_classes)

        self.text_projection = base_model.text_projection
        self.positional_embedding = base_model.positional_embedding
        self.ln_final = base_model.ln_final
        self.token_embedding = base_model.token_embedding

    def _freeze_layers(self, module, freeze_percentage, enc_name):
        named_params = list(module.named_parameters())
        cutoff = int(len(named_params) * freeze_percentage)
        print(f"Freezing first {freeze_percentage*100:.1f}% of {enc_name} ({cutoff}/{len(named_params)} tensors)")
        for i, (_, p) in enumerate(named_params):
            p.requires_grad = i >= cutoff

    def encode_text(self, token_ids):
        x = self.token_embedding(token_ids)
        x = x + self.positional_embedding
        x = x.permute(1, 0, 2)
        x = self.text_encoder(x)
        x = x.permute(1, 0, 2)
        x = self.ln_final(x)
        return x[torch.arange(x.shape[0]), token_ids.argmax(dim=-1)] @ self.text_projection

    def forward(self, images, text_inputs=None):
        image_features = self.visual_encoder(images)
        if text_inputs is not None:
            text_features = self.encode_text(text_inputs)
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
            return (image_features @ text_features.T) * 100.0
        return self.classifier(image_features)

# -----------------------------------------------------------------------------
# LoRA utilities
# -----------------------------------------------------------------------------
class LoRALinear(nn.Module):
    def __init__(self, linear, r=8, alpha=16, dropout=0.05):
        super().__init__()
        self.in_features = linear.in_features
        self.out_features = linear.out_features
        self.r = r
        self.scale = alpha / r
        self.dropout = nn.Dropout(p=dropout) if dropout > 0 else nn.Identity()
        # Use buffers so they move with .to() and never train
        self.register_buffer("weight", linear.weight.detach().clone())
        if linear.bias is not None:
            self.register_buffer("bias", linear.bias.detach().clone())
        else:
            self.bias = None
        self.lora_A = nn.Linear(self.in_features, r, bias=False)
        self.lora_B = nn.Linear(r, self.out_features, bias=False)
        nn.init.kaiming_uniform_(self.lora_A.weight, a=np.sqrt(5))
        nn.init.zeros_(self.lora_B.weight)

    def forward(self, x):
        base = nn.functional.linear(x, self.weight, self.bias)
        lora = self.lora_B(self.lora_A(self.dropout(x))) * self.scale
        return base + lora

def inject_lora_modules(root, target_substrings, r=8, alpha=16, dropout=0.05, parent_path=""):
    # Recursively replace Linear layers whose qualified name contains a target substring
    for child_name, child_module in list(root.named_children()):
        full_name = f"{parent_path}.{child_name}" if parent_path else child_name
        inject_here = any(s in full_name.lower() for s in target_substrings)
        if isinstance(child_module, nn.Linear) and inject_here:
            setattr(root, child_name, LoRALinear(child_module, r=r, alpha=alpha, dropout=dropout))
        else:
            inject_lora_modules(child_module, target_substrings, r=r, alpha=alpha, dropout=dropout, parent_path=full_name)

class LoRAClipBothEncoders(nn.Module):
    def __init__(self, base_model, subcategories, device, lora_r=8, lora_alpha=16):
        super().__init__()
        self.subcategories = subcategories
        self.device = device
        self.tokenizer = open_clip.get_tokenizer("ViT-B-32")

        # Components
        self.visual = base_model.visual
        self.text_encoder = base_model.transformer
        self.token_embedding = base_model.token_embedding
        self.positional_embedding = base_model.positional_embedding
        self.ln_final = base_model.ln_final
        self.text_projection = base_model.text_projection

        # Inject LoRA into attention/MLP/proj paths
        vis_targets = ["proj", "mlp", "attn"]
        txt_targets = ["proj", "mlp", "attn"]
        inject_lora_modules(self.visual, vis_targets, r=lora_r, alpha=lora_alpha)
        inject_lora_modules(self.text_encoder, txt_targets, r=lora_r, alpha=lora_alpha)

        # Ensure modules are on the correct device AFTER injection
        self.visual.to(self.device)
        self.text_encoder.to(self.device)

        # Freeze non-LoRA params
        for n, p in self.visual.named_parameters():
            if "lora_" not in n:
                p.requires_grad = False
        for n, p in self.text_encoder.named_parameters():
            if "lora_" not in n:
                p.requires_grad = False

    def encode_text(self, token_ids):
        x = self.token_embedding(token_ids)
        x = x + self.positional_embedding
        x = x.permute(1, 0, 2)
        x = self.text_encoder(x)
        x = x.permute(1, 0, 2)
        x = self.ln_final(x)
        return x[torch.arange(x.shape[0]), token_ids.argmax(dim=-1)] @ self.text_projection

    def forward(self, images, text_inputs=None):
        image_features = self.visual(images)
        if text_inputs is None:
            text_inputs = self.tokenizer([f"a photo of {c}" for c in self.subcategories]).to(self.device)
        text_features = self.encode_text(text_inputs)
        image_features = image_features / image_features.norm(dim=-1, keepdim=True)
        text_features = text_features / text_features.norm(dim=-1, keepdim=True)
        return (image_features @ text_features.T) * 100.0

class HfCLIPWithPEFT(nn.Module):
    def __init__(self, model_name, device, subcategories, lora_r=8, lora_alpha=16):
        super().__init__()
        self.clip = CLIPModel.from_pretrained(model_name)
        self.processor = CLIPProcessor.from_pretrained(model_name)
        self.subcategories = subcategories
        self.device = device
        self.clip = self.clip.to(device)

        # Configure LoRA
        target_modules = ["q_proj", "k_proj", "v_proj", "out_proj"]
        config = LoraConfig(
            r=lora_r,
            lora_alpha=lora_alpha,
            target_modules=target_modules,
            lora_dropout=0.05,
            bias="none",
            task_type=TaskType.FEATURE_EXTRACTION
        )

        # Apply LoRA to text model
        self.clip.text_model = get_peft_model(self.clip.text_model, config)
        self.clip.text_model.to(device)  # ensure device

        # Freeze non-LoRA params
        for n, p in self.clip.named_parameters():
            if "lora_" not in n:
                p.requires_grad = False

    def forward(self, images, text_inputs=None):
        image_features = self.clip.get_image_features(pixel_values=images)
        if text_inputs is None:
            text_inputs = self.processor.tokenizer(
                [f"a photo of {c}" for c in self.subcategories],
                padding=True,
                return_tensors="pt"
            ).to(self.device)

        outputs = self.clip.text_model(**text_inputs)
        last_hidden_state = outputs.last_hidden_state
        pooled = last_hidden_state[torch.arange(last_hidden_state.shape[0]),
                                   text_inputs["input_ids"].argmax(dim=-1)]
        text_features = self.clip.text_projection(pooled)

        image_features = image_features / image_features.norm(dim=-1, keepdim=True)
        text_features = text_features / text_features.norm(dim=-1, keepdim=True)
        return self.clip.logit_scale.exp() * (image_features @ text_features.t())

class FractionalFT_VisionCLIP(nn.Module):
    def __init__(self, base_model, num_classes, min_frac=0.01, max_frac=0.05):
        super().__init__()
        self.visual = base_model.visual
        self._set_fraction_trainable(min_frac, max_frac)

        out_dim = get_visual_out_dim(self.visual)
        self.head = nn.Sequential(
            nn.Linear(out_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, num_classes),
        )

    def _set_fraction_trainable(self, min_frac, max_frac):
        # Freeze all first
        for p in self.visual.parameters():
            p.requires_grad = False

        total = sum(p.numel() for p in self.visual.parameters())
        target_min = int(total * min_frac)
        target_max = int(total * max_frac)

        trainable = 0
        # Unfreeze from deeper layers backwards
        for name, module in reversed(list(self.visual.named_modules())):
            if any(isinstance(module, t) for t in [nn.Linear, nn.Conv2d, nn.LayerNorm]):
                for p in module.parameters():
                    if not p.requires_grad:
                        p.requires_grad = True
                        trainable += p.numel()
                if trainable >= target_min:
                    break

        # If overshot, freeze some layers
        while trainable > target_max:
            for name, module in self.visual.named_modules():
                if any(isinstance(module, t) for t in [nn.Linear, nn.Conv2d, nn.LayerNorm]):
                    if any(p.requires_grad for p in module.parameters()):
                        for p in module.parameters():
                            if p.requires_grad:
                                p.requires_grad = False
                                trainable -= p.numel()
                        break

        print(f"Fractional FT: {trainable}/{total} params trainable ({100*trainable/total:.2f}%)")

    def forward(self, images, text_inputs=None):
        return self.head(self.visual(images))

# -----------------------------------------------------------------------------
# Trainer
# -----------------------------------------------------------------------------
class Trainer:
    def __init__(self, device):
        self.device = device
        # GradScaler compatibility across torch versions
        try:
            # Newer API (positional device type)
            self.scaler = torch.amp.GradScaler('cuda', enabled=(device.type == "cuda"))
            self._autocast_ctx = (lambda: torch.amp.autocast('cuda')) if device.type == "cuda" else (lambda: nullcontext())
        except (TypeError, AttributeError):
            # Older API
            self.scaler = torch.cuda.amp.GradScaler(enabled=(device.type == "cuda"))
            self._autocast_ctx = (lambda: torch.cuda.amp.autocast()) if device.type == "cuda" else (lambda: nullcontext())

    def evaluate(self, model, loader, criterion, subcategories, fold, epoch, model_type, split_type, writer=None):
        model.eval()
        all_preds, all_labels, all_probs = [], [], []
        total_loss = 0.0
        texts = self._build_text_inputs(model, subcategories)

        with torch.no_grad():
            for images, labels in loader:
                images, labels = images.to(self.device), labels.to(self.device)
                outputs = model(images, text_inputs=texts) if texts is not None else model(images)
                loss = criterion(outputs, labels)
                total_loss += loss.item()

                probs = torch.softmax(outputs, dim=1)
                preds = torch.argmax(probs, dim=1)
                all_probs.append(probs.cpu().numpy())
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())

        avg_loss = total_loss / max(1, len(loader))
        acc = accuracy_score(all_labels, all_preds)
        precision, recall, f1, _ = precision_recall_fscore_support(
            all_labels, all_preds, average='weighted', zero_division=0)
        cm = confusion_matrix(all_labels, all_preds)

        self._log_metrics(writer, model_type, split_type, fold, epoch,
                          avg_loss, acc, precision, recall, f1, cm, all_probs, all_labels, subcategories)

        return avg_loss, acc, precision, recall, f1, cm

    def train(self, model, train_loader, val_loader, test_loader, criterion, optimizer,
              subcategories, fold, num_epochs, model_type, writer=None):
        best_f1 = 0.0
        patience = 3
        patience_counter = 0
        metrics = {
            "epoch": [],
            "train_loss": [], "train_acc": [],
            "val_loss": [], "val_acc": [],
            "test_loss": [], "test_acc": [],
            "precision_val": [], "recall_val": [], "f1_val": [],
            "precision_test": [], "recall_test": [], "f1_test": []
        }

        for epoch in range(num_epochs):
            model.train()
            total_loss, total_correct, total_samples = 0.0, 0, 0
            optimizer.zero_grad()
            texts = self._build_text_inputs(model, subcategories)

            for batch_idx, (images, labels) in enumerate(train_loader):
                images, labels = images.to(self.device), labels.to(self.device)

                with self._autocast_ctx():
                    outputs = model(images, text_inputs=texts) if texts is not None else model(images)
                    loss = criterion(outputs, labels) / Config.ACCUMULATION_STEPS

                self.scaler.scale(loss).backward()

                if (batch_idx + 1) % Config.ACCUMULATION_STEPS == 0:
                    if Config.CLIP_VALUE > 0:
                        clip_grad_value_(model.parameters(), Config.CLIP_VALUE)
                    self.scaler.step(optimizer)
                    self.scaler.update()
                    optimizer.zero_grad()

                    if writer is not None:
                        self._log_training_metrics(writer, model, optimizer, model_type, fold, epoch, batch_idx)

                total_loss += loss.item() * Config.ACCUMULATION_STEPS
                preds = torch.argmax(outputs, dim=1)
                total_correct += (preds == labels).sum().item()
                total_samples += labels.size(0)

            avg_loss = total_loss / max(1, len(train_loader))
            acc = total_correct / max(1, total_samples)

            if writer is not None:
                writer.add_scalar(f"{model_type}/Train/Loss", avg_loss, epoch + 1)
                writer.add_scalar(f"{model_type}/Train/Accuracy", acc, epoch + 1)

            # Validation and testing
            if (epoch + 1) % Config.VALIDATE_EVERY == 0 or epoch == num_epochs - 1:
                val_results = self.evaluate(model, val_loader, criterion, subcategories,
                                            fold, epoch + 1, model_type, "Validation", writer)
                test_results = self.evaluate(model, test_loader, criterion, subcategories,
                                             fold, epoch + 1, model_type, "Test", writer)

                metrics["epoch"].append(epoch + 1)
                metrics["train_loss"].append(avg_loss)
                metrics["train_acc"].append(acc)
                metrics["val_loss"].append(val_results[0])
                metrics["val_acc"].append(val_results[1])
                metrics["test_loss"].append(test_results[0])
                metrics["test_acc"].append(test_results[1])
                metrics["precision_val"].append(val_results[2])
                metrics["recall_val"].append(val_results[3])
                metrics["f1_val"].append(val_results[4])
                metrics["precision_test"].append(test_results[2])
                metrics["recall_test"].append(test_results[3])
                metrics["f1_test"].append(test_results[4])

                # Early stopping on best Validation F1
                if val_results[4] > best_f1:
                    best_f1 = val_results[4]
                    patience_counter = 0
                    self._save_model(model, model_type, fold)
                else:
                    patience_counter += 1
                    if patience_counter >= patience:
                        print(f"Early stopping at epoch {epoch + 1}")
                        break
            else:
                # Log train-only epoch
                metrics["epoch"].append(epoch + 1)
                metrics["train_loss"].append(avg_loss)
                metrics["train_acc"].append(acc)
                for k in ["val_loss", "val_acc", "test_loss", "test_acc",
                          "precision_val", "recall_val", "f1_val",
                          "precision_test", "recall_test", "f1_test"]:
                    metrics[k].append(0.0)

            print(f"Epoch {epoch + 1}/{num_epochs} | Loss: {avg_loss:.4f} | Acc: {acc:.4f}")

        return metrics

    def _build_text_inputs(self, model, subcategories):
        if not hasattr(model, "tokenizer"):
            return None
        prompts = [f"a photo of {c}" for c in subcategories]
        if hasattr(model, "processor"):  # HF CLIP
            return model.processor.tokenizer(prompts, padding=True, return_tensors="pt").to(self.device)
        else:  # OpenCLIP
            return model.tokenizer(prompts).to(self.device)

    def _log_metrics(self, writer, model_type, split_type, fold, epoch,
                     loss, acc, precision, recall, f1, cm, probs, labels, subcategories):
        if writer is None:
            return

        step = (fold - 1) * 1000 + (epoch + 1)

        # Scalars
        writer.add_scalar(f"{model_type}/{split_type}/Loss", loss, step)
        writer.add_scalar(f"{model_type}/{split_type}/Accuracy", acc, step)
        writer.add_scalar(f"{model_type}/{split_type}/Precision", precision, step)
        writer.add_scalar(f"{model_type}/{split_type}/Recall", recall, step)
        writer.add_scalar(f"{model_type}/{split_type}/F1", f1, step)

        # Confusion Matrix
        fig = plt.figure(figsize=(12, 10))
        sns.heatmap(cm, annot=False, fmt='d', cmap='Blues',
                    xticklabels=subcategories[:cm.shape[1]],
                    yticklabels=subcategories[:cm.shape[0]])
        plt.title(f'{model_type} {split_type} Confusion Matrix')
        plt.xlabel('Predicted')
        plt.ylabel('True')
        writer.add_figure(f"{model_type}/{split_type}/ConfusionMatrix", fig, step)
        plt.close(fig)

        # PR curves (top 5 classes by index)
        try:
            probs = np.concatenate(probs, axis=0)
            num_classes = probs.shape[1]
            labels_np = np.array(labels)
            for cls in range(min(5, num_classes)):
                writer.add_pr_curve(
                    f"{model_type}/{split_type}/PR_{subcategories[cls]}",
                    labels=(labels_np == cls).astype(int),
                    predictions=probs[:, cls],
                    global_step=step
                )
        except Exception as e:
            print(f"Could not log PR curves: {e}")

    def _log_training_metrics(self, writer, model, optimizer, model_type, fold, epoch, batch_idx):
        step = (fold - 1) * 1000 + (epoch + 1)

        # Learning rates
        for i, param_group in enumerate(optimizer.param_groups):
            writer.add_scalar(f"{model_type}/LR/group_{i}", param_group['lr'], step)

        # Gradient norm
        total_norm_sq = 0.0
        for p in model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm_sq += param_norm.item() ** 2
        total_norm = total_norm_sq ** 0.5
        writer.add_scalar(f"{model_type}/GradientNorm", total_norm, step)

        # Histograms periodically
        if batch_idx % (2 * Config.ACCUMULATION_STEPS) == 0:
            for name, param in model.named_parameters():
                if param.requires_grad:
                    writer.add_histogram(f"{model_type}/Params/{name}", param, step)
                    if param.grad is not None:
                        writer.add_histogram(f"{model_type}/Grads/{name}", param.grad, step)

    def _save_model(self, model, model_type, fold):
        os.makedirs(Config.BEST_DIR, exist_ok=True)
        path = os.path.join(Config.BEST_DIR, f"best_{model_type.lower().replace(' ', '_')}_fold{fold}.pth")
        torch.save(model.state_dict(), path)
        print(f"Saved best model to {path}")

# -----------------------------------------------------------------------------
# Main
# -----------------------------------------------------------------------------
def fresh_openclip(device):
    m, _, _ = open_clip.create_model_and_transforms("ViT-B-32", pretrained="openai")
    return m.to(device)

def main():
    freeze_support()
    start_time = time.time()
    print(f"Experiment started at {time.strftime('%Y-%m-%d %H:%M:%S')}")

    # Device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Data
    dataset = load_dataset("ceyda/fashion-products-small")['train']
    subcategories = sorted(set(item['subCategory'] for item in dataset))
    label_counts = np.bincount(
        [subcategories.index(item['subCategory']) for item in dataset],
        minlength=len(subcategories)
    )
    class_weights = torch.tensor(
        len(dataset) / (len(subcategories) * np.maximum(label_counts, 1))
    ).float().to(device)

    # Results (keys match Trainer.metrics)
    template_metrics = {
        "epoch": [],
        "train_loss": [], "train_acc": [],
        "val_loss": [], "val_acc": [],
        "test_loss": [], "test_acc": [],
        "precision_val": [], "recall_val": [], "f1_val": [],
        "precision_test": [], "recall_test": [], "f1_test": []
    }
    results = {
        "Full": copy.deepcopy(template_metrics),
        "Text": copy.deepcopy(template_metrics),
        "Vision": copy.deepcopy(template_metrics),
        "Partial": copy.deepcopy(template_metrics),
        "LoRA": copy.deepcopy(template_metrics),
        "PEFT": copy.deepcopy(template_metrics),
        "Fractional": copy.deepcopy(template_metrics),
    }
    timing = {k: [] for k in results.keys()}

    # Cross-validation
    kf = KFold(n_splits=Config.K_FOLDS, shuffle=True, random_state=42)
    for fold, (train_val_idx, test_idx) in enumerate(kf.split(dataset), start=1):
        print(f"\n=== Fold {fold}/{Config.K_FOLDS} ===")

        # Cast indices to Python ints for HF Datasets
        train_val_idx = to_pyints(train_val_idx)
        test_idx = to_pyints(test_idx)

        # Splits
        train_size = int(0.8 * len(train_val_idx))
        train_idx = train_val_idx[:train_size]
        val_idx = train_val_idx[train_size:]

        # Subsets
        train_subset = Subset(dataset, train_idx)
        val_subset = Subset(dataset, val_idx)
        test_subset = Subset(dataset, test_idx)

        # Datasets / loaders
        pin_mem = device.type == "cuda"
        num_workers = min(4, os.cpu_count() or 1)

        train_ds = FashionDataset(train_subset, subcategories, augment=True)
        val_ds = FashionDataset(val_subset, subcategories)
        test_ds = FashionDataset(test_subset, subcategories)

        train_loader = DataLoader(train_ds, batch_size=Config.BATCH_SIZE, shuffle=True,
                                  num_workers=num_workers, pin_memory=pin_mem)
        val_loader = DataLoader(val_ds, batch_size=Config.BATCH_SIZE, shuffle=False,
                                num_workers=num_workers, pin_memory=pin_mem)
        test_loader = DataLoader(test_ds, batch_size=Config.BATCH_SIZE, shuffle=False,
                                 num_workers=num_workers, pin_memory=pin_mem)

        # PEFT (HF) raw PIL path
        train_raw = FashionRawPILDataset(train_subset, subcategories)
        val_raw = FashionRawPILDataset(val_subset, subcategories)
        test_raw = FashionRawPILDataset(test_subset, subcategories)

        peft_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        peft_collate = CollateCLIP(peft_processor)

        peft_train_loader = DataLoader(train_raw, batch_size=Config.BATCH_SIZE, shuffle=True,
                                       collate_fn=peft_collate, num_workers=0, pin_memory=pin_mem)
        peft_val_loader = DataLoader(val_raw, batch_size=Config.BATCH_SIZE, shuffle=False,
                                     collate_fn=peft_collate, num_workers=0, pin_memory=pin_mem)
        peft_test_loader = DataLoader(test_raw, batch_size=Config.BATCH_SIZE, shuffle=False,
                                      collate_fn=peft_collate, num_workers=0, pin_memory=pin_mem)

        # Trainer / loss
        trainer = Trainer(device)
        criterion = nn.CrossEntropyLoss(weight=class_weights)

        # Fresh base models per variant (prevents cross-mutation and device drift)
        model_configs = [
            ("Full",
             FullFineTunedCLIP(fresh_openclip(device), len(subcategories)),
             optim.AdamW, {"lr": 1e-5, "weight_decay": 1e-4},
             train_loader, val_loader, test_loader),

            ("Text",
             TextEncoderFineTunedCLIP(fresh_openclip(device), subcategories, device),
             optim.AdamW, {"lr": 1e-4},
             train_loader, val_loader, test_loader),

            ("Vision",
             VisionOnlyFineTunedCLIP(fresh_openclip(device), len(subcategories)),
             optim.AdamW, {"lr": 1e-4},
             train_loader, val_loader, test_loader),

            ("Partial",
             PartialFineTunedCLIP(fresh_openclip(device), len(subcategories), 0.7),
             optim.AdamW, {"lr": 1e-4},
             train_loader, val_loader, test_loader),

            ("LoRA",
             LoRAClipBothEncoders(fresh_openclip(device), subcategories, device, 8, 16),
             optim.AdamW, {"lr": 2e-4},
             train_loader, val_loader, test_loader),

            ("PEFT",
             HfCLIPWithPEFT("openai/clip-vit-base-patch32", device, subcategories, 8, 16),
             optim.AdamW, {"lr": 2e-4},
             peft_train_loader, peft_val_loader, peft_test_loader),

            ("Fractional",
             FractionalFT_VisionCLIP(fresh_openclip(device), len(subcategories), 0.01, 0.05),
             optim.AdamW, {"lr": 1e-4, "weight_decay": 1e-4},
             train_loader, val_loader, test_loader),
        ]

        # Train each model
        for name, model, optimizer_class, optimizer_args, t_loader, v_loader, te_loader in model_configs:
            print(f"\nTraining {name} model...")
            model = model.to(device)

            writer = SummaryWriter(log_dir=os.path.join(Config.BASE_DIR, "tensorboard", f"{name}_fold{fold}"))

            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            writer.add_text(
                "ModelInfo",
                f"Total params: {total_params:,}\nTrainable params: {trainable_params:,}\n"
                f"Trainable %: {100*trainable_params/total_params:.2f}"
            )

            optimizer = optimizer_class([p for p in model.parameters() if p.requires_grad], **optimizer_args)

            start_t = time.time()
            metrics = trainer.train(
                model, t_loader, v_loader, te_loader, criterion, optimizer,
                subcategories, fold, Config.NUM_EPOCHS, name, writer
            )
            timing[name].append(time.time() - start_t)

            # Accumulate results
            for k in results[name]:
                results[name][k].extend(metrics[k])

            writer.close()

    # Save final results and plots
    save_results(results, timing)

    duration = time.time() - start_time
    print(f"\nExperiment completed in {duration//3600:.0f}h {(duration%3600)//60:.0f}m {duration%60:.0f}s")

# -----------------------------------------------------------------------------
# Results / Plots
# -----------------------------------------------------------------------------
def save_results(results, timing):
    # Save metrics to CSV
    for model_name, metrics in results.items():
        df = pd.DataFrame(metrics)
        df.to_csv(os.path.join(Config.CSV_DIR, f"{model_name}_metrics.csv"), index=False)

    # Save timing information
    timing_rows = []
    for name, times in timing.items():
        for i, t in enumerate(times, start=1):
            timing_rows.append({
                "Model": name,
                "Fold": i,
                "Time (s)": t,
                "Formatted": f"{int(t)//3600:02d}h {(int(t)%3600)//60:02d}m {int(t)%60:02d}s"
            })
    timing_df = pd.DataFrame(timing_rows)
    timing_df.to_csv(os.path.join(Config.CSV_DIR, "training_times.csv"), index=False)

    # Comparison plots
    compare_models(results)

def compare_models(results):
    metrics = ["val_loss", "val_acc", "f1_val"]  # fixed key name
    fig, axes = plt.subplots(len(metrics), 1, figsize=(12, 8 * len(metrics)))
    if len(metrics) == 1:
        axes = [axes]

    for ax, metric in zip(axes, metrics):
        for name, data in results.items():
            if metric in data and len(data[metric]) > 0:
                ax.plot(data[metric], label=name)
        ax.set_title(metric.replace("_", " ").title())
        ax.set_xlabel("Logged Step")
        ax.legend()
        ax.grid(True)

    plt.tight_layout()
    os.makedirs(Config.PNG_DIR, exist_ok=True)
    plt.savefig(os.path.join(Config.PNG_DIR, "model_comparison.png"))
    plt.close()

# -----------------------------------------------------------------------------
# Entry
# -----------------------------------------------------------------------------
if __name__ == "__main__":
    main()
